# -*- coding: utf-8 -*-
"""Wine Quality Data Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qL-IkJZLr4Z63_AKLy9Pyujv2PIwLZWc
"""

# Standard Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# Machine Learning
from sklearn.model_selection import train_test_split, KFold, cross_val_score,␣
↪GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
# Hypothesis Testing
from scipy.stats import ttest_ind
# Load Dataset
data = pd.read_csv('wine_quality.csv')
# Overview of Data
print(data.head())
print(data.info())
print(data.describe())
print("Missing Values:\n", data.isnull().sum())

## Exploratory Data Analysis (EDA)
#Neumerical Features
data.hist(bins=15, figsize=(15, 10), edgecolor='black')
plt.suptitle("Distributions of Numerical Features", fontsize=16)
plt.show()

# Correlations
# Heatmap of Correlations
plt.figure(figsize=(12, 8))
correlation_matrix = data.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Heatmap")
plt.show()

#Scattter Plot
# Scatter plot for Alcohol vs Quality
plt.figure(figsize=(8, 6))
sns.scatterplot(data=data, x="alcohol", y="quality", hue="quality" . palette="viridis")
plt.title("Alcohol vs. Quality")
plt.show()

#Pair Plot
selected_features = ['fixed acidity'
,
'volatile acidity'
'citric acid'
,
↪
'alcohol'
,
'quality']
sns.pairplot(data[selected_features], hue="quality", diag_kind="kde"
,␣
↪palette="coolwarm")
plt.suptitle("Pairplot of Selected Features", y=1.02, fontsize=16)
plt.show()

## Hypothesis Testing
high_quality = data[data['quality'] >= 7]['alcohol']
low_quality = data[data['quality'] < 7]['alcohol']
# Perform t-test
t_stat, p_value = ttest_ind(high_quality, low_quality)
print(f"T-statistic: {t_stat:.2f}, P-value: {p_value:.4f}")
if p_value < 0.05:
print("Significant difference in alcohol content between high and␣
↪low-quality wines.")
else:
print("No significant difference in alcohol content between high and␣
↪low-quality wines.")

## Machine Learning Models

# Data Preprocessing
# Split Features and Target
X = data.drop('quality', axis=1)
y = data['quality']
# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,␣
↪random_state=42)
# Standardize Data for Models that Require Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Linear Regression
lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred = lr.predict(X_test)
print("Linear Regression:")
print(f"Mean Squared Error: {mean_squared_error(y_test, y_pred):.2f}")
print(f"R² Score: {r2_score(y_test, y_pred):.2f}")

# K-Nearest Neighbors
knn = KNeighborsRegressor(n_neighbors=5)
knn.fit(X_train_scaled, y_train)
y_pred_knn = knn.predict(X_test_scaled)
print("KNN:")
print(f"Mean Squared Error: {mean_squared_error(y_test, y_pred_knn):.2f}")
print(f"R² Score: {r2_score(y_test, y_pred_knn):.2f}")

# Decision Tree
dt = DecisionTreeRegressor(max_depth=5, random_state=42)
dt.fit(X_train, y_train)
y_pred_dt = dt.predict(X_test)
print("Decision Tree:")
print(f"Mean Squared Error: {mean_squared_error(y_test, y_pred_dt):.2f}")
print(f"R² Score: {r2_score(y_test, y_pred_dt):.2f}")

# Random Forest
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
print("Random Forest:")
print(f"Mean Squared Error: {mean_squared_error(y_test, y_pred_rf):.2f}")
print(f"R² Score: {r2_score(y_test, y_pred_rf):.2f}")

# Gradient Boosting
gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,␣
↪max_depth=3, random_state=42)
gb.fit(X_train, y_train)
y_pred_gb = gb.predict(X_test)
print("Gradient Boosting:")
print(f"Mean Squared Error: {mean_squared_error(y_test, y_pred_gb):.2f}")
print(f"R² Score: {r2_score(y_test, y_pred_gb):.2f}")

# Neural Network
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
# Build Neural Network
model = Sequential([
Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),
Dropout(0.3),
Dense(64, activation='relu'),
Dense(1)
])
# Compile Model
model.compile(optimizer='adam', loss='mean_squared_error'
↪metrics=['mean_squared_error'])
,␣
# Train Model
history = model.fit(X_train_scaled, y_train, validation_data=(X_test_scaled,␣
↪y_test), epochs=50, batch_size=32, verbose=1)
# Evaluate Model
loss, mse_nn = model.evaluate(X_test_scaled, y_test, verbose=0)
print(f"Neural Network Mean Squared Error: {mse_nn:.2f}")

## Clustering Analysis
# KMeans Clustering
kmeans = KMeans(n_clusters=3, random_state=42)
X_scaled = scaler.fit_transform(X)
clusters = kmeans.fit_predict(X_scaled)
# Add Cluster Labels
data['Cluster'] = clusters
# PCA for Visualization
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
plt.figure(figsize=(8, 6))
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=clusters, palette="viridis")
plt.title("KMeans Clustering with PCA")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.show()

# Cross-Validation RMSE Summary Including Neural Network
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
# Standardize data for Neural Network
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
plt.title("KMeans Clustering with PCA")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.show()
[24]: # Cross-Validation RMSE Summary Including Neural Network
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
# Standardize data for Neural Network
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
16
models = {
'Linear Regression': LinearRegression(),
'KNN': KNeighborsRegressor(),
'Decision Tree': DecisionTreeRegressor(max_depth=5),
'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
'Gradient Boosting': GradientBoostingRegressor(n_estimators=100,␣
↪random_state=42),
'Neural Network': MLPRegressor(hidden_layer_sizes=(50, 25),␣
↪activation='relu', max_iter=1000, random_state=42)
}
cv_results = {}
kf = KFold(n_splits=10, shuffle=True, random_state=42)
for name, model in models.items():
if name == 'Neural Network':
cv_score = -cross_val_score(model, X_scaled, y,␣
↪scoring='neg_root_mean_squared_error', cv=kf)
else:
cv_score = -cross_val_score(model, X, y,␣
↪scoring='neg_root_mean_squared_error', cv=kf)
cv_results[name] = cv_score
print(f"{name} RMSE: {np.mean(cv_score):.3f} ± {np.std(cv_score):.3f}")
# Visualization of RMSE across Models (Boxplot)
plt.figure(figsize=(10, 6))
plt.boxplot(cv_results.values(), labels=cv_results.keys())
plt.title('Model Cross-Validation RMSE')
plt.ylabel('RMSE')
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
# Print Summary Table of Results
print("\nModel Performance Summary (Mean RMSE ± Std):")
for model_name, scores in cv_results.items():
print(f"{model_name}: {np.mean(scores):.3f} ± {np.std(scores):.3f}")